<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- HTML Meta Tags -->
  <title>Inversion-Free Image Editing with Natural Language</title>
  <meta name="description" content="Inversion-Free Image Editing with Natural Language">

  <!-- Facebook Meta Tags -->
  <meta property="og:url" content="https://sled-group.github.io/InfEdit/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Inversion-Free Image Editing with Natural Language">
  <meta property="og:description" content="Inversion-Free Image Editing with Natural Language">
  <meta property="og:image" content="https://sled-group.github.io/InfEdit/image_assets/preview.png">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta property="twitter:domain" content="sled-group.github.io">
  <meta property="twitter:url" content="https://sled-group.github.io/InfEdit/">
  <meta name="twitter:title" content="Inversion-Free Image Editing with Natural Language">
  <meta name="twitter:description" content="Inversion-Free Image Editing with Natural Language">
  <meta name="twitter:image" content="https://sled-group.github.io/InfEdit/image_assets/preview.png">


  <!-- Google tag (gtag.js) -->
  <script async="" src="./assets/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-9VZKE74FPW');
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } },
    });
  </script>
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

  <title>Inversion-Free Image Editing with Natural Language</title>

  <link href="./assets/css" rel="stylesheet">
  <link rel="stylesheet" href="./assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="./assets/css/bulma.min.css">
  <link rel="stylesheet" href="./assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./assets/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./assets/css/tab_gallery.css">
  <!-- <link rel="stylesheet" href="./assets/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="./assets/css/academicons.min.css">
  <link rel="stylesheet" href="./assets/css/index.css">
  <link rel="stylesheet" href="./assets/css/juxtapose.css">
  <link rel="stylesheet" href="./assets/css/image_card_fader.css">
  <link rel="stylesheet" href="./assets/css/image_card_slider.css">

  <link href='https://fonts.googleapis.com/css?family=Monoton' rel='stylesheet'>
  <link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet'>
  <link href='https://fonts.googleapis.com/css?family=Akronim' rel='stylesheet'>
  <link href='https://fonts.googleapis.com/css?family=Inconsolata' rel='stylesheet'>
  <link href='https://fonts.googleapis.com/css?family=Sofia ' rel='stylesheet'>
  <link href='https://fonts.googleapis.com/css?family=VT323 ' rel='stylesheet'>
  <link href='https://fonts.googleapis.com/css?family=Playfair Display ' rel='stylesheet'>

</head>



<body data-new-gr-c-s-check-loaded="14.1098.0" data-gr-ext-installed="">
  <section class="hero">
      <div class="hero-body">
          <div class="container is-max-desktop">
              <div class="columns is-centered">
                  <div class="column has-text-centered">
                      <h1 class="title is-1 publication-title">Inversion-Free Image Editing <br> with Natural Language</h1>
                      <div class="column has-text-centered">

                      </div>
                      <div class="is-size-5 publication-authors">
                          <p>
                            <span class="author-block">
                              <a href="https://sihanxu.github.io/">Sihan Xu</a><sup>1</sup>*&nbsp</span>
                              <span class="author-block">
                              <a href="https://h6kplus.github.io/owenhuang.github.io/">Yidong Huang</a><sup>1</sup>*&nbsp</span>
                              <span class="author-block">
                              <a href="https://www.jiayipan.me/">Jiayi Pan</a><sup>2</sup>&nbsp</span>
                              <span class="author-block">
                              <a href="http://ziqiaoma.com/">Ziqiao Ma</a><sup>1&infin;</sup>&nbsp</span>
                              <span class="author-block">
                              <a href="https://web.eecs.umich.edu/~chaijy/">Joyce Chai</a><sup>1</sup>&nbsp</span>
                          </p>
                      </div>
                      <div class="is-size-5 publication-authors">
                      <p>
                          <span class="author-block">
                          <sup>1</sup>University of Michigan&nbsp</span>
                          <span class="author-block">
                          <sup>2</sup>University of California, Berkeley&nbsp</span>
                       </p>
                      <p id="fn1" style="font-size: medium;">* Equal contribution &nbsp&nbsp <sup>&infin;</sup> Correspondence
                          <a href="#ref2"></a>
                      </p>
                      </div>
                      <div class="publication-links">
                        <span class="link-block">
                            <a href="https://sihanxu.github.io/InfEdit/docs/infedit.pdf" class="external-link button is-normal is-rounded is-dark">
                              <span class="icon">
                                <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg="">
                                  <path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
                                  </path>
                                </svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
                              </span>
                              <span>Paper</span>
                            </a>
                          </span>
                        <span class="link-block">
                            <a href="https://arxiv.org/" class="external-link button is-normal is-rounded is-dark">
                              <span class="icon">
                                <i class="ai ai-arxiv"></i>
                              </span>
                              <span>ArXiv</span>
                            </a>
                          </span>
                          <span class="link-block">
                            <a href="https://github.com/sled-group/InfEdit" class="external-link button is-normal is-rounded is-dark">
                              <span class="icon">
                                <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg="">
                                  <path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
                                  </path>
                                </svg>
                              </span>
                              <span>Code</span>
                            </a>
                          </span>
                        <span class="link-block">
                        <a href="http://sled-snowbird.eecs.umich.edu/" class="external-link button is-normal is-rounded is-dark">
                            <span>ðŸ¤—Demo</span>
                        </a>
                        </span>
                        <span class="link-block">
                        <a href="#bibtex" class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                              <i class="ai ai-obp"></i>
                            </span>
                            <span>BibTex</span>
                          </a>
                        </span>
                    </br>
                        <span class="link-block">
                            <a href="https://github.com/sled-group/InfEdit/tree/website" class="external-link button is-normal is-rounded is-dark">
                                <span>User Handbook (Under Construction)</span>
                              </a>
                            </span>
                    </div>
                  </div>
              </div>
              <div class="has-text-centered">
                  <img src="./image_assets/infedit_gif.gif" width="100%">
                  <!-- <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
          <source src="./video_assets/workflow_demo.mp4" type="video/mp4">
        </video> -->
                  <!-- <iframe width="960" height="540" src="https://www.youtube.com/embed/ihDbAUh0LXk?si=ASIzJd_Q2Tq1pwjq" 
        title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
              </div>
              <div class="column has-text-centered link-container" id="blocks">
                  <div class="publication-links">
                    <span class="link-block">
                      <a href="./index.html#blocks" class="external-link button is-normal" style="width: 24%; color: #3273DC;">
                        Home
                      </a>
                    </span>
                    <span class="link-block">
                      <a href="./method.html#blocks" class="external-link button is-normal" style="width: 24%; background-color: #2780e3; color: aliceblue;">
                        Method
                      </a>
                    </span>
                    <span class="link-block">
                      <a href="./compare.html#blocks" class="external-link button is-normal" style="width: 24%; color: #3273DC;">
                        Compare
                      </a>
                    </span>
                    <span class="link-block">
                      <a href="./gallery.html#blocks" class="external-link button is-normal" style="width: 24%; color: #3273DC;">
                        Gallery
                      </a>
                    </span>
                  </div>
              </div>
          </div>
      </div>
  </section>

  <section class="section">

    <!--/ Matting. -->
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4 has-text-justified">Background</h2>
          <div class="content has-text-justified">
              <h4>
                Diffusion Models
              </h4>
          </div>

          <p>
            Diffusion models (DMs) operate through a forward process that gradually adds Gaussian noises to data, described as follows:
            \[ \begin{equation}z_{t} = \sqrt{{\alpha}_t} z_0 + \sqrt{1 - {\alpha}_t} \varepsilon\quad \varepsilon \sim \mathcal{N}(0,\mathcal{I}) \end{equation} \]
          </p>
          <p>
            where \(z_0\) is a sample from the data distribution, \({\alpha}_{1:T}\) specify a variance schedule for \(t\sim [1, T]\).
            The training objective involves a parameterized noise prediction network, \(\varepsilon_{\theta}\), which aims to reverse the diffusion process. 
            The training objective is to minimize the following loss based on a chosen metric function for measuring the distance between two samples \(d(\cdot, \cdot)\):
            \[ \begin{equation}\min_\theta \mathbb{E}_{z_0, \varepsilon, t} \big[d\left(\varepsilon, \varepsilon_{\theta}(z_t,t)\right)\big] \end{equation} \]
          </p>
          
          <p>
            Sampling from a diffusion model is an iterative process that progressively denoises the data. 
            Following Eq (12) in <a href="https://arxiv.org/abs/2010.02502">Song et al. (2020)</a>, the denoising step at \( t \) is formulated as:
            \[ \begin{equation}
            \begin{aligned}
                z_{t-1} = 
                & \sqrt{{{\alpha}}_{t-1}} \left( \frac{z_t - \sqrt{1 - {{\alpha}}_t} \varepsilon_{\theta}(z_t, t)}{\sqrt{{\alpha}}_t} \right) \quad \text{(predicted } z_0 \text{)} \\
                & + \sqrt{1 - {\alpha}_{t-1} - \sigma_t^2} \cdot \varepsilon_{\theta}(z_t,t) \quad \text{(direction to } z_t \text{)} \\
                & + \sigma_t \varepsilon_t\quad \text{where } \varepsilon_t \sim \mathcal{N}(0,\mathcal{I}) \quad \text{(random noise)}
            \end{aligned}
            \end{equation}
            \]
          </p>

          <p>
            DDPM sampling introduces a noise schedule \(\sigma_t\) so that Eq (3) becomes Markovian.
            By setting \(\sigma_t\) to vanish, DDIM sampling results in an implicit probabilistic model with a deterministic forward process.
          </p>

          <p>
            Following DDIM, we can use the function \(f_\theta\) to predict and reconstruct \(\bar{z_0}\) given \(z_t\):
            \[
            \begin{equation}
            \bar{z}_0 
            = f_\theta(z_t, t) 
            = \left(z_t - \sqrt{1 - {\alpha}_t} \cdot \varepsilon_\theta(z_t, t) \right) / \sqrt{{\alpha}_t}
            \end{equation}
            \]
          </p>

          <p>
            Recently, Latent Diffusion Models (LDMs) offer a new paradigm by operating in the latent space. 
            The source latent \(z_0\) is acquired by encoding a sample \(x_0\) with an encoder \(\mathcal{E}\), such that \(z_0 = \mathcal{E}(x_0)\). 
            So as to be reversed, the output can then be reconstructed by a decoder \(\mathcal{D}\).
            This framework presents a computationally efficient way to generate high-fidelity images, as the diffusion process is conducted in a latent space with lower dimensions.
          </p>
          <br>
          <div class="content has-text-justified">
            <h4>
              Consistency Models
            </h4>
          </div>

          <p>
            Consistency models (CMs) have recently been introduced, which greatly accelerate the generation process compared with previous DMs.
            One notable property of CMs is self-consistency, such that samples along a trajectory map to the sample initial. 
            The key is a consistency function \(f(z_t, t)\), which ensures a consistent distillation process by optimizing:
            \[
            \begin{equation}
            \min_{\theta,\theta^{-};\phi} \mathbb{E}_{z_0,t} \left[d\left(f_{\theta}(z_{t_{n+1}}, t_{n+1}), f_{\theta^{-}}(\hat{z}^{\phi}_{t_{n}}, t_{n})\right)\right]
            \end{equation}
            \]
          </p>
          
          <p>
            in which \(f_{\theta}\) denotes a trainable neural network that parameterizes these consistent transitions, while \(f_{\theta^{-}}\) represents a slowly updated target model used for consistency distillation, with the update rule \(\theta^- \leftarrow \mu \theta ^{-} + (1-\mu) \theta\) given a decay rate \(\mu|). 
            The variable \(\hat{z}^\phi_{t_n}\) denotes a one-step estimation of \(z_{t_n}\) from \(z_{t_{n+1}}\).
          </p>

          <p>
            Sampling in CMs is carried out through a sequence of timesteps \(\tau_{1:n} \in [t_0,T]\). 
            Starting from an initial noise \(\hat{z}_T\) and \(z_0^{(T)} = f_\theta(\hat{z}_T, T)\), at each time-step \(\tau_{i}\), the process samples \(\varepsilon \sim \mathcal{N}(0, \mathcal{I})\) and iteratively updates the Multistep Consistency Sampling process:

            \[
            \begin{equation}
            \begin{aligned}
            \hat{z}_{\tau_i} &= z_0^{(\tau_{i+1})} + \sqrt{\tau_i^2 - t_0^2 }\varepsilon \\
            z_0^{(\tau_{i})} &= f_{\theta}(\hat{z}_{\tau_i}, \tau_i)
            \end{aligned}
            \end{equation}
            \]
          </p>
          
          <p>
            Latent Consistency Models (LCMs) extend to accommodate a (text) condition \(c\), which is crucial for text-guided image manipulation. 
            Similarly, sampling in LCMs at \(\tau_{i}\) starts with \(\varepsilon \sim \mathcal{N}(0, \mathcal{I})\) and updates:
            \[
            \begin{equation}
            \begin{aligned}
            \label{eq:lcm-samp}
            % \hat{z}_{\tau_i} &= \sqrt{{\alpha}_{\tau_i}}z_0^{(\tau_{i+1})} + \sqrt{1-{\alpha}_{\tau_i}} \varepsilon, \\
            \hat{z}_{\tau_i} &= \sqrt{{\alpha}_{\tau_i}}z_0^{(\tau_{i+1})} + \sigma_{\tau_i} \varepsilon, \\
            z_0^{(\tau_{i})} &= f_{\theta}(\hat{z}_{\tau_i}, \tau_i, c)
            \end{aligned}
            \end{equation}
            \]
          <br>
          </p>
            <div class="content has-text-justified">
              <h4>
                Inversion-Based Image Editing with LDMs
              </h4>
            </div>
          <p>
            DDIM inversion is effective for unconditional diffusion applications, but lacks consistency with additional text or image conditions.
            As illustrated in Figure 1a, the predicted \(\bar{z}_0'\) deviates from the original source \(z_0\), cumulatively leading to undesirable semantic changes.
            This substantially restricts its use in image editing driven by natural language-guided diffusion.
          </p>
          <p>
            To address this concern, various forms of inversion-based editing methods have been proposed.
            The predominating approaches utilize optimization-based inversion.
            These methods aim to "correct" the forward latents guided by the source prompt (referred to as the source branch) by aligning them with the DDIM inversion trajectory.
            To tackle the efficiency bottlenecks and suboptimal consistency, very recent work has explored dual-branch inversion.
            These methods separate the source and target branches in the editing process: directly revert the source branch back to \(z_0\) and iteratively calibrate the trajectory of the target branch.
            As shown in Figure 2a, they calculate the distance between the source branch and the inversion branch (or directly sampled from q-sampling in CycleDiffusion), 
            and calibrate the target branch with this computed distance at each \(t\).
          </p>
          <div class="content has-text-centered">
            <img src="./image_assets/method/inversion.png" width="150%">
              <p class="has-text-centered">
                <b>Figure 1. </b>While DDIM is prone to reconstruction error and requires iterative inversion, DDCM accepts any random noise to start with. 
                It introduces a non-Markovian forward process in which \(z_t\) directly points to the ground truth \(z_0\) without neural prediction, 
                and \(z_{t-1}\) does not depend on the previous step \(z_t\) like a consistency model. 
              </p>
          </div>

          
        </div>
      </div>

    </div>
  </section>

  <section class="section">

    <!--/ Matting. -->
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4 has-text-justified">Denoising Diffusion Consistent Models</h2>
          We start with the following proposition.
          <p>
            <b>Proposition 1 (Denoising Diffusion Consistent Models)</b>
            Consider a special case of Eq (3) when \(\sigma_t\) is chosen as \(\sqrt{1 - \alpha_{t-1}}\) across all time \(t\), the forward process naturally aligns with the Multistep (Latent) Consistency Sampling.
          </p>
          <p>
            When \(\sigma_t=\sqrt{1 - \alpha_{t-1}}\), the second term of Eq (3) vanishes:
            \[
            \begin{equation}
            \begin{aligned}
                z_{t-1} = 
                & \sqrt{{{\alpha}}_{t-1}} \left( \frac{z_t - \sqrt{1 - {{\alpha}}_t} \varepsilon_{\theta}(z_t, t)}{\sqrt{{\alpha}}_t} \right) && \text{(predicted $z_0$)} \\
                & + \sqrt{1 - \alpha_{t-1}} \varepsilon_t\quad \varepsilon_t \sim \mathcal{N}(0,\mathcal{I}) && \text{(random noise)}
            \end{aligned}
            \end{equation}
            \]
          </p>
          <p>
            Consider \(f(z_t, t; z_0) = \left( z_t - \sqrt{1 - {\alpha}_t} \varepsilon'(z_t,t;z_0) \right) / \sqrt{{\alpha}_t}\), 
            where the initial \(z_0\) is available (which is the case for image editing applications) and we replace the parameterized noise predictor \(\varepsilon_\theta\) with \(\varepsilon'\) more generally.
            Eq (8) becomes
            \[
            \begin{equation}
            \begin{aligned}
                z_{t-1} = \sqrt{{{\alpha}}_{t-1}} f(z_t,t;z_0) + \sqrt{1 - \alpha_{t-1}} \varepsilon_t
            \end{aligned}
            \end{equation}
            \]
            which is in the same form as the Multistep Latent Consistency Sampling step in Eq (7).
          </p>
          <p>
            In order to make \(f(z_t, t)\) self-consistent so that it can be considered as a consistency function, i.e., \(f(z_t, t; z_0) = z_0\), 
            we can directly solve the equation and \(\varepsilon'\) can be computed without parameterization:
            \[
            \begin{equation}
            \varepsilon^{\text{cons}} = \varepsilon'(z_t, t;z_0) = \frac{z_t - \sqrt{{\alpha}_t} z_0}{\sqrt{1 - {\alpha}_t}}
            \end{equation}
            \]
          </p>
          <p>
            As illustrated in Figure 1c, we arrive at a non-Markovian forward process, in which \(z_t\) directly points to the ground truth \(z_0\) without neural prediction, 
            and \(z_{t-1}\) does not depend on the previous step \(z_t\) like a consistency model.
            We name this <b>Denoising Diffusion Consistent Model</b> (DDCM).
          </p>
          <br>
          <div class="content has-text-justified">
              <h4>
                DDCM for Virtual Inversion
              </h4>
          </div>
          <p>
            We note that DDCM suggests an image reconstruction model without any explicit inversion operation, diverging from conventional DDIM inversion and its optimized or calibrated variations for image editing. 
            It achieves the best efficiency as it allows the forward process to start from any random noise and supports multi-step consistency sampling. 
            On the other hand, it ensures exact consistency between original and reconstructed images as each step on the forward branch \(z_{t-1}\) only depends on the ground truth \(z_0\) rather than the previous step \(z_t\). 
            Due to its inversion-free nature, we name this method <b>Virtual Inversion</b>.
            As outlined in Algorithm 1, \(z = z_0\) is ensured throughout the process without parameterization.
          </p>
          <div class="content has-text-centered">
            <img src="./image_assets/method/algo1.png" width="50%">
          </div>
          <br>
          <div class="content has-text-justified">
              <h4>
                DDCM for Inversion-Free Image Editing
              </h4>
          </div>
          <p>
            Existing inversion-based editing methods are limited for real-time and real-world language-driven image editing applications. 
            First, most of them still depend on a time-consuming inversion process to obtain the inversion branch as a set of anchors.
            Second, consistency remains a bottleneck given the efforts from optimization and calibration.
            Recall that dual-branch inversion methods perform editing on the target branch by iteratively calibrating the \({z}_t^\textrm{tgt}\) 
            with the actual distance between the source branch and the inversion branch at \(t\), as is boxed in Figure 3a.
            While they ensure faithful reconstruction by leaving the source branch untouched from the target branch, 
            the calibrated \(z_t^\textrm{tgt}\) does guarantee consistency from \(z_t^\textrm{src}\) in the source branch, 
            as can be seen from the visible difference between \({z}_0^\textrm{src}\) and \({z}_0^\textrm{tgt}\) in Figure 3a.
            Third, all current inversion-based methods rely on variations of diffusion sampling, which are incompatible with efficient Consistency Sampling using LCMs.
          </p>
          <div class="content has-text-centered">
            <img src="./image_assets/method/edit.png" width="150%">
            <p class="has-text-centered">
              <b>Figure 2. </b>A comparative overview of the dual-branch inversion editing and inversion-free editing with DDCM. 
              We initialize \(z_0^\textrm{tgt}\) with \(z_0^\textrm{src}\) in DDCM sampling for visualization purposes, while in principle it can start from any random noise. 
              The circled numbers correspond to Algorithm 2.
            </p>
          </div>
          <p>
            DDCM offers an alternative to address these limitations, introducing an Inversion-Free Image Editing (InfEdit) framework.
            While also adopting a dual-branch paradigm, the key of our InfEdit method is to directly calibrate the initial \({z}_0^\textrm{tgt}\) rather than the \({z}_t^\textrm{tgt}\) along the branch, 
            as is boxed in Figure 3b.
            InfEdit starts from a random terminal noise \(z^{\textrm{src}}_{\tau_1} = z^{\textrm{tgt}}_{\tau_1} \sim \mathcal{N}(0,\mathcal{I})\).
            As shown in Figure 3b, the source branch follows the DDCM sampling process without explicit inversion, 
            and we directly compute the distance \(\Delta\varepsilon^\textrm{cons}\) between \(\varepsilon^\textrm{cons}\) the \(\varepsilon_\theta^\textrm{src}\)
            (the predicted noise to reconstruct a \(\bar{z}_0^\textrm{src}\)).
            For the target branch, we first compute the \(\varepsilon_\theta^\textrm{tgt}\) to predict \(\bar{z}_0^\textrm{tgt}\), 
            and then calibrate the predicted target initial with the same \(\Delta\varepsilon^\textrm{cons}\).
            Algorithm 2 outlines the mathematical details of this process, 
            in which we slightly abuse the notation to define \(f_\theta(z_t,t,\varepsilon) = \left( z_t - \sqrt{1 - {\alpha}_t} \varepsilon \right) / \sqrt{{\alpha}_t}\).
          </p>
          <div class="content has-text-centered">
            <img src="./image_assets/method/algo2.png" width="50%">
          </div>
          <p>
            InfEdit addresses the current limitations of inversion-based editing methods.
            First, DDCM sampling allows us to abandon the inversion branch anchors required by previous methods, saving a significant amount of computation.
            Second, the current dual-branch methods calibrate \(z_t^\textrm{tgt}\) over time, while InfEdit directly refines the predicted initial \(z_0^\textrm{tgt}\), 
            without suffering from the cumulative errors over the course of sampling.
            Third, our framework is compatible with efficient Consistency Sampling using LCMs, enabling efficient sampling of the target image within very few steps.
          </p>

        </div>
      </div>

    </div>
  </section>

  <section class="section">

    <!--/ Matting. -->
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-4 has-text-justified">Unifying Attention Control for Language-Guided Editing</h2>
          <p>
          InfEdit suggests a general inversion-free framework for image editing motivated by DDCM.
          In the realm of language-driven editing, achieving a nuanced understanding of the language condition and facilitating finer-grained interaction across modalities becomes a challenge.
          Prompt-to-Prompt noticed that the interaction between the text and image modalities occurs in the parameterized noise prediction network \(\varepsilon_\theta\), 
          and opened up a series of attention control methods to compute a noise \(\widehat{\varepsilon_\theta^\textrm{tgt}}\) that more accurately aligns with the language prompts.
          In the context of InfEdit specifically, attention control refines the original predicted target noise \(\varepsilon_\theta^\textrm{tgt}\) 
          (noted in â‘£ in Algorithm 2 and Figure 2b with \(\widehat{\varepsilon_\theta^\textrm{tgt}}\). 
          </p>
          <p>
            We follow Prompt-to-Prompt (P2P) in terms of notation.
            Each basic block of the U-Net noise predictor contains a cross-attention module and a self-attention module.
            The spatial features are linearly projected into queries (\(Q\)).
            In cross-attention, the text features are linearly projected into keys (\(K\)) and values (\(V\)).
            In self-attention, the keys (\(K\)) and values (\(V\)) are also obtained from linearly projected spatial features.
            The attention mechanism can be given as:
            \[
            \begin{equation}
            \text{Attention}(K,Q,V) = MV = \textrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
            \end{equation}
            \]
            in which \(M_{i,j}\) represents the attention map that determines the weight to aggregate the value of the \(j\)-th token on pixel \(i\), and \(d\) denotes the dimension for \(K\) and \(Q\).
          </p>

          <p>
            Natural language specifies a wide spectrum of semantic changes.
            In the following, we describe how <br> <b>rigid semantic changes</b>, e.g., those on the visual features and background, 
            can be controlled via cross attention; and how <br> <b>non-rigid semantic changes</b>, e.g., those leading to adding/removing an object, 
            novel action manners and physical state changes of objects, can be controlled via mutual self-attention.
            We then introduce a Unified Attention Control (UAC) protocol for both types of semantic changes.
          </p>
          <br>
          <div class="content has-text-justified">
              <h4>
                Cross-Attention Control
              </h4>
          </div> 
          <p>
            Prompt-to-Prompt (P2P) observed that cross-attention layers can capture the interaction between the spatial structures of pixels and words in the prompts, even in early steps.
            This finding makes it possible to control the cross-attention for editing rigid semantic changes, simply by replacing the cross-attention map of generated images with that of the original images.
          </p>
          <p>
            <b>Global Attention Refinement: </b>
            At time step \(t\), we compute the attention map \(M_{t}\) averaged over layers given the noised latent \(z_t\) and the prompt for both source and target branch.
            We drop the time step for simplicity and represent the source and target attention maps as \(M^{\textrm{src}}\) and \(M^{\textrm{tgt}}\).
            To represent the common details, an alignment function \(A(i) = j\) is introduced which signifies that the \(i^{\textrm{th}}\) word in the target prompt corresponds 
            to the \(j^{\textrm{th}}\) word in the source prompt.
            Following P2P, we refine the target attention map by injecting the source attention map over the common tokens.
            \[
            \begin{equation}
            \textrm{Refine}(M^{\textrm{src}},M^{\textrm{tgt}})_{i,j}
            = \begin{cases}
                \left(M^{\textrm{tgt}}\right)_{i,j} 
                & \text{if} A(j)=\text{None} \\ 
                \left(M^{\textrm{src}}\right)_{i,A(j)} 
                & \text{otherwise}
            \end{cases}
            \end{equation}
            \]
            This ensures that the common information from the source prompt is accurately transferred to the target, while the requested changes are made.
          </p>
          <p>
            <b>Local Attention Blends: </b>
            Besides global attention refinement, we adapt the blended diffusion mechanism from Blended Diffusion and Prompt-to-Prompt (P2P).
            Specifically, the algorithm takes optional inputs of target blend words \(w^{\textrm{tgt}}\), which are words in the target prompt whose semantics need to be added; 
            and source blend words \(w^{\textrm{src}}\), which are words in the source prompt whose semantics need to be preserved. 
            At time step \(t\), we blend the noised target latent \(z_t^{\textrm{tgt}}\) following:
            \begin{equation}
            \begin{aligned}
                m^{\textrm{tgt}} &= \text{Threshold} \big[M_t^{\textrm{tgt}}(w^{\textrm{tgt}}), a^{\textrm{tgt}}\big] \\
                m^{\textrm{src}} &= \text{Threshold} \big[M_t^{\textrm{src}}(w^{\textrm{src}}), a^{\textrm{src}}\big] \\
                z_t^{\textrm{tgt}} &= (1-m^{\textrm{tgt}}+m^{\textrm{src}}) \odot z_t^{\textrm{src}} + (m^{\textrm{tgt}}-m^{\textrm{src}}) \odot z_t^{\textrm{tgt}}  \\
            \end{aligned}
            \end{equation}
          </p>
          <p>
            in which \(m^{\textrm{tgt}}\) and \(m^{\textrm{src}}\) are binary masks obtained by calibrating the aggregated attention maps 
            \(M_t^{\textrm{tgt}}(w^{\textrm{tgt}}),M_t^{\textrm{src}}(w^{\textrm{src}})\) with threshold parameters \(a^{\textrm{tgt}}\) and \(a^{\textrm{src}}\).
          </p>
          <p>
            <b>Scheduling Cross-Attention Control: </b>
            Applying cross-attention control throughout the entire sampling schedule will overly focus on spatial consistency, leading to an inability to capture the intended changes.
            Follow P2P, we perform cross-attention control only in early steps before \(\tau_c\), interpreted as the cross-attention control strength:
            \[
            \begin{equation}
            \textrm{CrossEdit}(M^{\textrm{src}},M^{\textrm{tgt}},t):=\begin{cases}\textrm{Refine}(M^{\textrm{src}},M^{\textrm{tgt}}) &t \geq \tau_c \\ M^{\textrm{tgt}} & t < \tau_c\end{cases}
            \end{equation}
            \]
          </p>
          <br>
          <div class="content has-text-justified">
              <h4>
                Mutual Self-Attention Control
              </h4>
          </div>
          <p>
            One key limitation of cross-attention control lies in its inability in non-rigid editing. 
            Instead of applying controls over the cross-attention modules, MasaCtrl observed that the layout of the objects can be roughly formed in the self-attention queries, 
            covering the non-rigid semantic changes complying with the target prompt. 
            The core idea is to synthesize the structural layout with the target prompt in the early steps with the original \(Q^{\textrm{tgt}}, K^{\textrm{tgt}}, V^{\textrm{tgt}}\) in the self-attention; 
            and then to query semantically similar contents in \(K^{\textrm{src}}, V^{\textrm{src}}\) with the target query \(Q^{\textrm{tgt}}\).
          </p>
          <p>
            <b>Controlling Non-Rigid Semantic Changes: </b>
            MasaCtrl suffers from the issue of undesirable non-rigid changes.
            As shown in results, MasaCtrl can lead to significant inconsistency from the source images, 
            especially in terms of the composition of objects and when there are multiple objects and complex backgrounds.
            This is not surprising, as the target query \(Q^{\textrm{tgt}}\) is used throughout the self-attention control schedule.
            Instead of relying on the target prompts to guide the premature steps, we form the structural layout with the source self-attention 
            \(Q^{\textrm{src}}, K^{\textrm{src}}, V^{\textrm{src}}\) in the self-attention.
            We show in results that this design enables high-quality non-rigid changes while maintaining satisfying structural consistency.
          </p>
          <p>
            <b>Scheduling Mutual Self-Attention Control: </b>
            This mutual self-attention control is applied in the later steps after \(\tau_s\), interpreted as the mutual self-attention control strength:
            \[
            \begin{equation}
            \begin{aligned}
                \textrm{SelfEdit}(\{Q^{\textrm{src}},K^{\textrm{src}},V^{\textrm{src}}\},\{Q^{\textrm{tgt}},K^{\textrm{tgt}},V^{\textrm{tgt}}\},t) := \\
                \begin{cases}\{Q^{\textrm{src}},K^{\textrm{src}},V^{\textrm{src}}\}&t \geq \tau_s \\ \{Q^{\textrm{tgt}},K^{\textrm{src}},V^{\textrm{src}}\} & t < \tau_s\end{cases}\\
            \end{aligned}
            \end{equation}
            \]
          </p>
          <br>
          <div class="content has-text-justified">
              <h4>
                Unified Attention Control
              </h4>
          </div>
          <div class="content has-text-centered">
            <img src="./image_assets/method/UAC.png" style="width: 60%; margin-right: 5%"> <img src="./image_assets/method/algo3.png" width="30%">
            <p class="has-text-centered">
              <b>Figure 3. </b>The proposed United Attention Control (UAC) framework to unify cross-attention control and mutual self-attention control. 
              UAC introduces an additional layout branch as an intermediate to host the desired composition and structural information in the target image.
            </p>
          </div>
          <p>
            The UAC framework is detailed in Algorithm 3 and illustrated in Figure 3.
            During each forward step of the diffusion process, UAC starts with mutual self-attention control on \(z^{\textrm{src}}\) and \(z^{\textrm{tgt}}\) and assigns the output to the layout branch latent \(z^{\textrm{lay}}\). 
            Following this, cross-attention control is applied on \(M^{\textrm{lay}}\) and \(M^{\textrm{tgt}}\) to refine the semantic information for \(M^{\textrm{tgt}}\). 
            As is shown in Figure 3, the layout branch output \(z_0^{\textrm{lay}}\) reflects the requested non-rigid changes (e.g., "standing"), while preserving the non-rigid content semantics (e.g., ``brown'').
            The target branch output \(z_0^{\textrm{tgt}}\) builds upon the structural layout of the \(z_0^{\textrm{lay}}\) while reflecting the requested non-rigid changes (e.g., "green").
          </p>
        </div>
      </div>

    </div>
  </section>
  
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title"><a id="bibtex">BibTeX</a></h2>
        <pre><code>@ TO BE UPDATED</code></pre>
    </div>
</section>

  <footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>

  </footer>


  <script type="text/javascript">
    var currentMethodList = ['ours', 'plain', 'full', 'aae', 'ip2p', 'p2p'];
    var currentMethod = "ours";

    function ChangeMethod(idx) {
      var li_list = document.getElementById("method-view-ul").children;
      for (i = 0; i < li_list.length; i++) {
        li_list[i].className = "nav-link";
      }
      li_list[idx].className = "nav-link active";
      currentMethod = currentMethodList[idx];

      var image_livingroom = document.getElementById('image_livingroom');
      var image_cityscape = document.getElementById('image_cityscape');
      image_livingroom.src = "./image_assets/footnote/livingroom/livingroom_" + currentMethod + ".jpg";
      image_cityscape.src = "./image_assets/footnote/cityscape/cityscape_" + currentMethod + ".jpg";
    }
    function ChangePrompt(idx) {
      var li_list = document.getElementById("prompt-view-ul").children;
      for (i = 0; i < li_list.length; i++) {
        li_list[i].className = "nav-link";
      }
      li_list[idx].className = "nav-link active";
      var div_livingroom = document.getElementById('div_livingroom');
      var div_cityscape = document.getElementById('div_cityscape');
      if (idx == 0) {
        div_livingroom.style.display = "";
        div_cityscape.style.display = "none";
      } else {
        div_livingroom.style.display = "none";
        div_cityscape.style.display = "";
      }
    }
  </script>

  <script src="./assets/js/juxtapose.js"></script>
  <script src="./assets/js/popper.min.js"></script>
  <script src="./assets/js/jquery.min.js"></script>
  <script src="./assets/js/bootstrap.min.js"></script>
  <script defer="" src="./assets/js/fontawesome.all.min.js"></script>
  <script src="./assets/js/bulma-carousel.min.js"></script>
  <script src="./assets/js/bulma-slider.min.js"></script>
  <script src="./assets/js/magnifier.js"></script>

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>

</html>
